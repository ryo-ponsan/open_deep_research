{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rlm/Desktop/Code/open_deep_research/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rlm/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U -q open-deep-research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.10\n"
     ]
    }
   ],
   "source": [
    "import open_deep_research   \n",
    "print(open_deep_research.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.types import Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from open_deep_research.graph import builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "# display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# Set the API keys used for any model or search tool selections below, such as:\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"GROQ_API_KEY\")\n",
    "_set_env(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writer_model_name: claude-3-5-sonnet-latest\n",
      "model='claude-3-5-sonnet-latest' anthropic_api_url='https://api.anthropic.com' anthropic_api_key=SecretStr('**********') model_kwargs={}\n",
      "{}\n",
      "<class 'langchain_anthropic.chat_models.ChatAnthropic'>\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for enhanced_tavily_search\n  Input should be a valid dictionary or instance of enhanced_tavily_search [type=model_type, input_value=['Fireworks Together.ai G...antages infrastructure'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverview of the AI inference market with focus on Fireworks, Together.ai, Groq\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Run the graph until the interruption\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mastream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m:topic,}, thread, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__interrupt__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[1;32m     57\u001b[0m         interrupt_value \u001b[38;5;241m=\u001b[39m event[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__interrupt__\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2305\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2299\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   2302\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   2304\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 2305\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   2306\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   2307\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2308\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   2309\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2310\u001b[0m     ):\n\u001b[1;32m   2311\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   2313\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/pregel/runner.py:444\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    442\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[1;32m    445\u001b[0m         t,\n\u001b[1;32m    446\u001b[0m         retry_policy,\n\u001b[1;32m    447\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    448\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    449\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[1;32m    450\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[1;32m    451\u001b[0m         },\n\u001b[1;32m    452\u001b[0m     )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/pregel/retry.py:128\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/utils/runnable.py:583\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    580\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langgraph/utils/runnable.py:371\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    370\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 371\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    373\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/src/open_deep_research/graph.py:97\u001b[0m, in \u001b[0;36mgenerate_report_plan\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     94\u001b[0m query_list \u001b[38;5;241m=\u001b[39m [query\u001b[38;5;241m.\u001b[39msearch_query \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mqueries]\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Search the web with parameters\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m source_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m select_and_execute_search(search_api, query_list, params_to_pass)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Format system instructions\u001b[39;00m\n\u001b[1;32m    100\u001b[0m system_instructions_sections \u001b[38;5;241m=\u001b[39m report_planner_instructions\u001b[38;5;241m.\u001b[39mformat(topic\u001b[38;5;241m=\u001b[39mtopic, report_organization\u001b[38;5;241m=\u001b[39mreport_structure, context\u001b[38;5;241m=\u001b[39msource_str, feedback\u001b[38;5;241m=\u001b[39mfeedback)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/src/open_deep_research/utils.py:1220\u001b[0m, in \u001b[0;36mselect_and_execute_search\u001b[0;34m(search_api, query_list, params_to_pass)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Select and execute the appropriate search API.\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;124;03m    ValueError: If an unsupported search API is specified\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m search_api \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtavily\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;66;03m# Tavily search tool used with workflow and agent \u001b[39;00m\n\u001b[0;32m-> 1220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m enhanced_tavily_search\u001b[38;5;241m.\u001b[39mainvoke(query_list)\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m search_api \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1222\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m perplexity_search(query_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams_to_pass)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langchain_core/tools/structured.py:66\u001b[0m, in \u001b[0;36mStructuredTool.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoroutine:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# If the tool does not implement async, fall back to default implementation\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langchain_core/tools/base.py:523\u001b[0m, in \u001b[0;36mBaseTool.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    521\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    522\u001b[0m     tool_input, kwargs \u001b[38;5;241m=\u001b[39m _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marun(tool_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langchain_core/tools/base.py:887\u001b[0m, in \u001b[0;36mBaseTool.arun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_tool_error(error_to_raise)\n\u001b[0;32m--> 887\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    889\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langchain_core/tools/base.py:844\u001b[0m, in \u001b[0;36mBaseTool.arun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    842\u001b[0m error_to_raise: Optional[Union[\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 844\u001b[0m     tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langchain_core/tools/base.py:651\u001b[0m, in \u001b[0;36mBaseTool._to_args_and_kwargs\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_schema, \u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m ):\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;66;03m# StructuredTool with no args\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (), {}\n\u001b[0;32m--> 651\u001b[0m tool_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# For backwards compatibility, if run_input is a string,\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;66;03m# pass as a positional argument.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_input, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/langchain_core/tools/base.py:570\u001b[0m, in \u001b[0;36mBaseTool._parse_input\u001b[0;34m(self, tool_input, tool_call_id)\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    569\u001b[0m             tool_input[k] \u001b[38;5;241m=\u001b[39m tool_call_id\n\u001b[0;32m--> 570\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minput_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m     result_dict \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(input_args, BaseModelV1):\n",
      "File \u001b[0;32m~/Desktop/Code/open_deep_research/open-deep-research-env/lib/python3.11/site-packages/pydantic/main.py:627\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    626\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for enhanced_tavily_search\n  Input should be a valid dictionary or instance of enhanced_tavily_search [type=model_type, input_value=['Fireworks Together.ai G...antages infrastructure'], input_type=list]\n    For further information visit https://errors.pydantic.dev/2.10/v/model_type",
      "\u001b[0mDuring task with name 'generate_report_plan' and id 'b27025b8-b1b6-79b9-0629-d2a17fa5d6bd'"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "from IPython.display import Markdown\n",
    "\n",
    "REPORT_STRUCTURE = \"\"\"Use this structure to create a report on the user-provided topic:\n",
    "\n",
    "1. Introduction (no research needed)\n",
    "   - Brief overview of the topic area\n",
    "\n",
    "2. Main Body Sections:\n",
    "   - Each section should focus on a sub-topic of the user-provided topic\n",
    "   \n",
    "3. Conclusion\n",
    "   - Aim for 1 structural element (either a list of table) that distills the main body sections \n",
    "   - Provide a concise summary of the report\"\"\"\n",
    "\n",
    "# Claude 3.7 Sonnet for planning with perplexity search\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"perplexity\",\n",
    "                           \"planner_provider\": \"anthropic\",\n",
    "                           \"planner_model\": \"claude-3-7-sonnet-latest\",\n",
    "                           # \"planner_model_kwargs\": {\"temperature\":0.8}, # if set custom parameters\n",
    "                           \"writer_provider\": \"anthropic\",\n",
    "                           \"writer_model\": \"claude-3-5-sonnet-latest\",\n",
    "                           # \"writer_model_kwargs\": {\"temperature\":0.8}, # if set custom parameters\n",
    "                           \"max_search_depth\": 2,\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           }}\n",
    "\n",
    "# DeepSeek-R1-Distill-Llama-70B for planning and llama-3.3-70b-versatile for writing\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"tavily\",\n",
    "                           \"planner_provider\": \"groq\",\n",
    "                           \"planner_model\": \"deepseek-r1-distill-llama-70b\",\n",
    "                           \"writer_provider\": \"groq\",\n",
    "                           \"writer_model\": \"llama-3.3-70b-versatile\",\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           \"max_search_depth\": 1,}\n",
    "                           }\n",
    "\n",
    "# Fast config (less search depth) with o3-mini for planning and Claude 3.5 Sonnet for writing\n",
    "thread = {\"configurable\": {\"thread_id\": str(uuid.uuid4()),\n",
    "                           \"search_api\": \"tavily\",\n",
    "                           \"planner_provider\": \"openai\",\n",
    "                           \"planner_model\": \"o3-mini\",\n",
    "                           \"writer_provider\": \"anthropic\",\n",
    "                           \"writer_model\": \"claude-3-5-sonnet-latest\",\n",
    "                           \"max_search_depth\": 1,\n",
    "                           \"report_structure\": REPORT_STRUCTURE,\n",
    "                           }}\n",
    "\n",
    "# Create a topic\n",
    "topic = \"Overview of the AI inference market with focus on Fireworks, Together.ai, Groq\"\n",
    "\n",
    "# Run the graph until the interruption\n",
    "async for event in graph.astream({\"topic\":topic,}, thread, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in event:\n",
    "        interrupt_value = event['__interrupt__'][0].value\n",
    "        display(Markdown(interrupt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Please provide feedback on the following report plan. \n",
       "                        \n",
       "\n",
       "Section: Introduction\n",
       "Description: Provides a brief overview of the AI inference market and introduces the key players: Fireworks, Together.ai, and Groq.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "Section: Market Overview and Context\n",
       "Description: Examines the overall AI inference market landscape, trends driving technology adoption, and market positioning of various AI infrastructure providers.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Deep Dive: Fireworks\n",
       "Description: Focuses on Fireworks' platform, highlighting its inference performance, cost-effectiveness, and pricing model, alongside revenue estimates (ARR) and market impact for startups and developers.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Deep Dive: Together.ai\n",
       "Description: Analyzes Together.ai with emphasis on its capabilities for high-performance AI training, enterprise security, and advanced customization, including an overview of revenue estimates (ARR) and its competitive positioning.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Deep Dive: Groq\n",
       "Description: Evaluates Groq’s technology focusing on its inference speed, cost factors, tokenomics, and performance metrics, including revenue estimates (ARR) and its deployment strategy in the market.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Comparative Analysis and ARR Estimates\n",
       "Description: Presents a side-by-side comparison of Fireworks, Together.ai, and Groq, focusing on performance, pricing, scalability, and revenue (ARR) metrics to highlight their unique strengths and market positions.\n",
       "Research needed: Yes\n",
       "\n",
       "\n",
       "Section: Conclusion\n",
       "Description: Summarizes the key findings and insights from the report into a concise summary table, distilling the main points from the in-depth analysis of each provider.\n",
       "Research needed: No\n",
       "\n",
       "\n",
       "                        \n",
       "Does the report plan meet your needs?\n",
       "Pass 'true' to approve the report plan.\n",
       "Or, provide feedback to regenerate the report plan:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass feedback to update the report plan  \n",
    "async for event in graph.astream(Command(resume=\"Include individuals sections for Together.ai, Groq, and Fireworks with revenue estimates (ARR)\"), thread, stream_mode=\"updates\"):\n",
    "    if '__interrupt__' in event:\n",
    "        interrupt_value = event['__interrupt__'][0].value\n",
    "        display(Markdown(interrupt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'human_feedback': None}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Market Overview and Context', description='Examines the overall AI inference market landscape, trends driving technology adoption, and market positioning of various AI infrastructure providers.', research=True, content=\"## Market Overview and Context\\n\\nThe AI inference market is experiencing remarkable growth, driven by increasing investments in AI technologies and the rising adoption across industries. According to market projections, the total addressable market for AI-related hardware and software is expected to grow between 40% and 55% annually over the next three years, potentially reaching $780-990 billion by 2027 [1].\\n\\nThe market landscape is evolving rapidly with diverse players offering specialized solutions. Traditional GPU manufacturers like NVIDIA continue to dominate, while newer entrants like Fireworks AI, Together AI, and Groq are disrupting the space with innovative approaches [2]. Fireworks AI has positioned itself as a cost-effective solution for startups and developers, particularly excelling in chat models and image generation, while Together AI focuses on high-performance training and enterprise security [3].\\n\\nRecent innovations in the market include edge computing solutions and advanced inference systems designed for real-time insights. The growth is particularly fueled by the increasing demand for processing large volumes of image data in real-time and the expansion of generative AI applications [4]. As the market matures, competition is intensifying around key performance metrics like throughput, latency, and cost-effectiveness [5].\\n\\n### Sources\\n[1] AI's Trillion-Dollar Opportunity: https://www.bain.com/insights/ais-trillion-dollar-opportunity-tech-report-2024/\\n[2] AI Inference Market Forecast Report to 2030: https://finance.yahoo.com/news/ai-inference-market-forecast-report-082100788.html\\n[3] Fireworks AI vs Together AI: AI Performance, Scalability, and Cost: https://koonka.ai/fireworks-ai-vs-together-ai/\\n[4] AI Inference Server Market Size, Share | CAGR of 18.40%: https://market.us/report/ai-inference-server-market/\\n[5] ArtificialAnalysis.ai LLM Benchmark Doubles Axis - Groq: https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Deep Dive: Fireworks', description=\"Focuses on Fireworks' platform, highlighting its inference performance, cost-effectiveness, and pricing model, alongside revenue estimates (ARR) and market impact for startups and developers.\", research=True, content=\"## Deep Dive: Fireworks\\n\\nFireworks AI has emerged as a significant player in the AI inference market since its 2022 launch, experiencing remarkable growth that led to a $552 million valuation by mid-2024 [1]. The company's platform stands out for its high-performance inference capabilities, powered by its proprietary FireAttention inference engine that supports text, image, and audio processing while maintaining HIPAA and SOC2 compliance [2].\\n\\nThe platform operates on a flexible pay-as-you-go model, charging based on token usage for serverless inference, GPU time for on-demand deployments, and training data volume for fine-tuning [3]. Their model offerings demonstrate impressive performance metrics, with Llama 3.1 8B and Mixtral 8x7B leading in output speed at 193 and 171 tokens per second respectively [4].\\n\\nThe company has shown strong market traction, with traffic increasing 100x within six months of launch. Their latest $52 million Series B funding round, led by Sequoia, brought total funding to $77 million [5]. Fireworks has demonstrated practical value for developers, as evidenced by Cursor's implementation of their custom Llama 3-70b model achieving 1000 tokens/sec for code generation use cases [6].\\n\\n### Sources\\n[1] https://entrepreneurialtales.com/how-this-ai-startup-secured-a-552m-valuation-backed-by-nvidia/\\n[2] https://www.helicone.ai/blog/llm-api-providers\\n[3] https://docs.fireworks.ai/faq/billing-pricing-usage/pricing/cost-structure\\n[4] https://artificialanalysis.ai/providers/fireworks\\n[5] https://tracxn.com/d/companies/fireworks/__ucWeLWhllYG-tQw61ZYBnPmuZYGc-t6Njqo1qsxplTQ\\n[6] https://fireworks.ai/blog/fireworks-ai-series-b-compound-ai\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Comparative Analysis and ARR Estimates', description='Presents a side-by-side comparison of Fireworks, Together.ai, and Groq, focusing on performance, pricing, scalability, and revenue (ARR) metrics to highlight their unique strengths and market positions.', research=True, content=\"## Comparative Analysis and ARR Estimates\\n\\nA detailed performance comparison reveals distinct advantages for each provider in the AI inference market. Groq stands out with exceptional speed metrics, achieving 241 tokens per second for Llama 2 Chat (70B), more than double the speed of many competitors [1]. More recent benchmarks show even higher performance, with Groq's specialized Tensor Streaming Processor (TSP) delivering 500-700 tokens/second on large language models, representing a 5-10x improvement over NVIDIA's data center GPUs [2].\\n\\nFireworks AI positions itself as the most cost-effective option for inference, particularly for models like Mixtral 8x7B and Llama 3 8B. It excels in providing fast and affordable inference services, making it especially attractive for startups and developers seeking efficient solutions [3].\\n\\nTogether.ai demonstrates strong performance in latency metrics, achieving impressive Time To First Token (TTFT) scores of 0.33s with their Turbo offering for Llama 3.3 70B [4]. While Cerebras and Groq show superior output speeds with their custom hardware, factors like TTFT and total response time create a more complex performance picture, particularly when handling longer context lengths [5].\\n\\n### Sources\\n[1] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n[2] https://sacra.com/c/groq/\\n[3] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[4] https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers\\n[5] https://medium.com/friendliai/a-comparative-analysis-of-ai-api-providers-based-on-llama-3-1-70b-a9d89c52bfd1\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Deep Dive: Groq', description='Evaluates Groq’s technology focusing on its inference speed, cost factors, tokenomics, and performance metrics, including revenue estimates (ARR) and its deployment strategy in the market.', research=True, content=\"## Deep Dive: Groq\\n\\nGroq has emerged as a significant player in the AI inference market, distinguished by its groundbreaking LPU (Linear Processing Unit) Inference Engine technology. In benchmark tests, Groq's LPU demonstrated exceptional performance, achieving over 300 tokens per second per user on the Llama 2 70B model, outperforming other cloud providers by up to 18x in output tokens throughput [1][2]. Independent benchmarks by ArtificialAnalysis.ai confirmed Groq's superior performance, recording 241 tokens per second and 0.8-second response time for 100 output tokens [3].\\n\\nFounded in 2016 by former Google engineers, Groq has seen substantial growth, recently securing $640 million in Series D funding [4]. The company's current implied valuation stands at $3.6 billion, marking a 219% increase from its Series C valuation [5]. With an estimated annual revenue of $46.6 million and 315 employees [6], Groq is rapidly scaling its operations.\\n\\nGroq's deployment strategy includes plans to install over 108,000 LPUs manufactured by GlobalFoundries by Q1 2025, representing the largest AI inference compute deployment outside of hyperscalers [7]. The company has already attracted over 360,000 developers to GroqCloud™, where they can access various open models including Llama 3.1, Whisper Large V3, and Mixtral [7].\\n\\n### Sources\\n[1] https://medium.com/@giladam01/why-meta-ais-llama-3-running-on-groq-s-lpu-inference-engine-sets-a-new-benchmark-for-large-2da740415773\\n[2] https://groq.com/groq-lpu-inference-engine-crushes-first-public-llm-benchmark/\\n[3] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n[4] https://aimagazine.com/machine-learning/groq-the-ai-chip-startup-worth-us-2-8bn\\n[5] https://www.pminsights.com/insights/groq-seeks-more-capital-and-continues-to-establish-itself-as-a-key-ai-player\\n[6] https://compworth.com/company/groq\\n[7] https://groq.com/news_press/groq-raises-640m-to-meet-soaring-demand-for-fast-ai-inference/\")]}}\n",
      "\n",
      "\n",
      "{'build_section_with_web_research': {'completed_sections': [Section(name='Deep Dive: Together.ai', description='Analyzes Together.ai with emphasis on its capabilities for high-performance AI training, enterprise security, and advanced customization, including an overview of revenue estimates (ARR) and its competitive positioning.', research=True, content=\"## Deep Dive: Together.ai\\n\\nTogether.ai has emerged as a significant player in the AI infrastructure space, focusing on simplifying enterprise use of open-source large language models (LLMs). The company's growth has been remarkable, achieving $100M in annual recurring revenue (ARR) in less than 10 months and reaching an estimated $130M ARR in 2024, representing a 400% year-over-year increase [1][2].\\n\\nThe company's valuation has seen impressive growth, rising to $3.3 billion following recent funding rounds. Together.ai has secured total funding of $228.5 million across three rounds, demonstrating strong investor confidence [3][4]. CEO Vipul Ved Prakash emphasizes their focus on combining state-of-the-art open source models with high-performance infrastructure and advanced AI efficiency research [5].\\n\\nUnlike traditional GPU cloud providers, Together.ai benefits from the commoditization of compute, as falling GPU prices lower their cost basis while maintaining competitive per-token pricing. The company differentiates itself through superior developer experience and reliable inference across a wide range of open-source models [2]. Their platform specifically caters to enterprises seeking AI infrastructure solutions that support model customization and deployment [4].\\n\\n### Sources\\n[1] https://www.arr.club/signal/together-ai-arr-at-100m-in-less-than-10-months\\n[2] https://sacra.com/c/together-ai/\\n[3] https://tracxn.com/d/companies/together-ai/__fcIBLE0rJMeK3FAdcfzE0H41jE36bJd0FDBWalYo6cY\\n[4] https://www.allaboutai.com/ai-news/together-ai-reaches-3-3b-dollars-valuation-general-catalyst-led-funding/\\n[5] https://www.forbes.com/sites/esatdedezade/2025/02/21/together-ais-valuation-soars-to-33-billion-as-demand-for-ai-computing-grows/\")]}}\n",
      "\n",
      "\n",
      "{'gather_completed_sections': {'report_sections_from_research': \"\\n============================================================\\nSection 1: Market Overview and Context\\n============================================================\\nDescription:\\nExamines the overall AI inference market landscape, trends driving technology adoption, and market positioning of various AI infrastructure providers.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Market Overview and Context\\n\\nThe AI inference market is experiencing remarkable growth, driven by increasing investments in AI technologies and the rising adoption across industries. According to market projections, the total addressable market for AI-related hardware and software is expected to grow between 40% and 55% annually over the next three years, potentially reaching $780-990 billion by 2027 [1].\\n\\nThe market landscape is evolving rapidly with diverse players offering specialized solutions. Traditional GPU manufacturers like NVIDIA continue to dominate, while newer entrants like Fireworks AI, Together AI, and Groq are disrupting the space with innovative approaches [2]. Fireworks AI has positioned itself as a cost-effective solution for startups and developers, particularly excelling in chat models and image generation, while Together AI focuses on high-performance training and enterprise security [3].\\n\\nRecent innovations in the market include edge computing solutions and advanced inference systems designed for real-time insights. The growth is particularly fueled by the increasing demand for processing large volumes of image data in real-time and the expansion of generative AI applications [4]. As the market matures, competition is intensifying around key performance metrics like throughput, latency, and cost-effectiveness [5].\\n\\n### Sources\\n[1] AI's Trillion-Dollar Opportunity: https://www.bain.com/insights/ais-trillion-dollar-opportunity-tech-report-2024/\\n[2] AI Inference Market Forecast Report to 2030: https://finance.yahoo.com/news/ai-inference-market-forecast-report-082100788.html\\n[3] Fireworks AI vs Together AI: AI Performance, Scalability, and Cost: https://koonka.ai/fireworks-ai-vs-together-ai/\\n[4] AI Inference Server Market Size, Share | CAGR of 18.40%: https://market.us/report/ai-inference-server-market/\\n[5] ArtificialAnalysis.ai LLM Benchmark Doubles Axis - Groq: https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n\\n\\n============================================================\\nSection 2: Deep Dive: Fireworks\\n============================================================\\nDescription:\\nFocuses on Fireworks' platform, highlighting its inference performance, cost-effectiveness, and pricing model, alongside revenue estimates (ARR) and market impact for startups and developers.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Deep Dive: Fireworks\\n\\nFireworks AI has emerged as a significant player in the AI inference market since its 2022 launch, experiencing remarkable growth that led to a $552 million valuation by mid-2024 [1]. The company's platform stands out for its high-performance inference capabilities, powered by its proprietary FireAttention inference engine that supports text, image, and audio processing while maintaining HIPAA and SOC2 compliance [2].\\n\\nThe platform operates on a flexible pay-as-you-go model, charging based on token usage for serverless inference, GPU time for on-demand deployments, and training data volume for fine-tuning [3]. Their model offerings demonstrate impressive performance metrics, with Llama 3.1 8B and Mixtral 8x7B leading in output speed at 193 and 171 tokens per second respectively [4].\\n\\nThe company has shown strong market traction, with traffic increasing 100x within six months of launch. Their latest $52 million Series B funding round, led by Sequoia, brought total funding to $77 million [5]. Fireworks has demonstrated practical value for developers, as evidenced by Cursor's implementation of their custom Llama 3-70b model achieving 1000 tokens/sec for code generation use cases [6].\\n\\n### Sources\\n[1] https://entrepreneurialtales.com/how-this-ai-startup-secured-a-552m-valuation-backed-by-nvidia/\\n[2] https://www.helicone.ai/blog/llm-api-providers\\n[3] https://docs.fireworks.ai/faq/billing-pricing-usage/pricing/cost-structure\\n[4] https://artificialanalysis.ai/providers/fireworks\\n[5] https://tracxn.com/d/companies/fireworks/__ucWeLWhllYG-tQw61ZYBnPmuZYGc-t6Njqo1qsxplTQ\\n[6] https://fireworks.ai/blog/fireworks-ai-series-b-compound-ai\\n\\n\\n============================================================\\nSection 3: Deep Dive: Together.ai\\n============================================================\\nDescription:\\nAnalyzes Together.ai with emphasis on its capabilities for high-performance AI training, enterprise security, and advanced customization, including an overview of revenue estimates (ARR) and its competitive positioning.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Deep Dive: Together.ai\\n\\nTogether.ai has emerged as a significant player in the AI infrastructure space, focusing on simplifying enterprise use of open-source large language models (LLMs). The company's growth has been remarkable, achieving $100M in annual recurring revenue (ARR) in less than 10 months and reaching an estimated $130M ARR in 2024, representing a 400% year-over-year increase [1][2].\\n\\nThe company's valuation has seen impressive growth, rising to $3.3 billion following recent funding rounds. Together.ai has secured total funding of $228.5 million across three rounds, demonstrating strong investor confidence [3][4]. CEO Vipul Ved Prakash emphasizes their focus on combining state-of-the-art open source models with high-performance infrastructure and advanced AI efficiency research [5].\\n\\nUnlike traditional GPU cloud providers, Together.ai benefits from the commoditization of compute, as falling GPU prices lower their cost basis while maintaining competitive per-token pricing. The company differentiates itself through superior developer experience and reliable inference across a wide range of open-source models [2]. Their platform specifically caters to enterprises seeking AI infrastructure solutions that support model customization and deployment [4].\\n\\n### Sources\\n[1] https://www.arr.club/signal/together-ai-arr-at-100m-in-less-than-10-months\\n[2] https://sacra.com/c/together-ai/\\n[3] https://tracxn.com/d/companies/together-ai/__fcIBLE0rJMeK3FAdcfzE0H41jE36bJd0FDBWalYo6cY\\n[4] https://www.allaboutai.com/ai-news/together-ai-reaches-3-3b-dollars-valuation-general-catalyst-led-funding/\\n[5] https://www.forbes.com/sites/esatdedezade/2025/02/21/together-ais-valuation-soars-to-33-billion-as-demand-for-ai-computing-grows/\\n\\n\\n============================================================\\nSection 4: Deep Dive: Groq\\n============================================================\\nDescription:\\nEvaluates Groq’s technology focusing on its inference speed, cost factors, tokenomics, and performance metrics, including revenue estimates (ARR) and its deployment strategy in the market.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Deep Dive: Groq\\n\\nGroq has emerged as a significant player in the AI inference market, distinguished by its groundbreaking LPU (Linear Processing Unit) Inference Engine technology. In benchmark tests, Groq's LPU demonstrated exceptional performance, achieving over 300 tokens per second per user on the Llama 2 70B model, outperforming other cloud providers by up to 18x in output tokens throughput [1][2]. Independent benchmarks by ArtificialAnalysis.ai confirmed Groq's superior performance, recording 241 tokens per second and 0.8-second response time for 100 output tokens [3].\\n\\nFounded in 2016 by former Google engineers, Groq has seen substantial growth, recently securing $640 million in Series D funding [4]. The company's current implied valuation stands at $3.6 billion, marking a 219% increase from its Series C valuation [5]. With an estimated annual revenue of $46.6 million and 315 employees [6], Groq is rapidly scaling its operations.\\n\\nGroq's deployment strategy includes plans to install over 108,000 LPUs manufactured by GlobalFoundries by Q1 2025, representing the largest AI inference compute deployment outside of hyperscalers [7]. The company has already attracted over 360,000 developers to GroqCloud™, where they can access various open models including Llama 3.1, Whisper Large V3, and Mixtral [7].\\n\\n### Sources\\n[1] https://medium.com/@giladam01/why-meta-ais-llama-3-running-on-groq-s-lpu-inference-engine-sets-a-new-benchmark-for-large-2da740415773\\n[2] https://groq.com/groq-lpu-inference-engine-crushes-first-public-llm-benchmark/\\n[3] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n[4] https://aimagazine.com/machine-learning/groq-the-ai-chip-startup-worth-us-2-8bn\\n[5] https://www.pminsights.com/insights/groq-seeks-more-capital-and-continues-to-establish-itself-as-a-key-ai-player\\n[6] https://compworth.com/company/groq\\n[7] https://groq.com/news_press/groq-raises-640m-to-meet-soaring-demand-for-fast-ai-inference/\\n\\n\\n============================================================\\nSection 5: Comparative Analysis and ARR Estimates\\n============================================================\\nDescription:\\nPresents a side-by-side comparison of Fireworks, Together.ai, and Groq, focusing on performance, pricing, scalability, and revenue (ARR) metrics to highlight their unique strengths and market positions.\\nRequires Research: \\nTrue\\n\\nContent:\\n## Comparative Analysis and ARR Estimates\\n\\nA detailed performance comparison reveals distinct advantages for each provider in the AI inference market. Groq stands out with exceptional speed metrics, achieving 241 tokens per second for Llama 2 Chat (70B), more than double the speed of many competitors [1]. More recent benchmarks show even higher performance, with Groq's specialized Tensor Streaming Processor (TSP) delivering 500-700 tokens/second on large language models, representing a 5-10x improvement over NVIDIA's data center GPUs [2].\\n\\nFireworks AI positions itself as the most cost-effective option for inference, particularly for models like Mixtral 8x7B and Llama 3 8B. It excels in providing fast and affordable inference services, making it especially attractive for startups and developers seeking efficient solutions [3].\\n\\nTogether.ai demonstrates strong performance in latency metrics, achieving impressive Time To First Token (TTFT) scores of 0.33s with their Turbo offering for Llama 3.3 70B [4]. While Cerebras and Groq show superior output speeds with their custom hardware, factors like TTFT and total response time create a more complex performance picture, particularly when handling longer context lengths [5].\\n\\n### Sources\\n[1] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n[2] https://sacra.com/c/groq/\\n[3] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[4] https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers\\n[5] https://medium.com/friendliai/a-comparative-analysis-of-ai-api-providers-based-on-llama-3-1-70b-a9d89c52bfd1\\n\\n\"}}\n",
      "\n",
      "\n",
      "{'write_final_sections': {'completed_sections': [Section(name='Introduction', description='Provides a brief overview of the AI inference market and introduces the key players: Fireworks, Together.ai, and Groq.', research=False, content=\"# The AI Inference Market: Examining Fireworks, Together.ai, and Groq\\n\\nThe AI inference market is experiencing unprecedented growth, driven by increasing enterprise adoption and technological innovations. As the market expands at an estimated 40-55% annually toward a projected $780-990 billion valuation by 2027, new players are challenging traditional GPU manufacturers with specialized solutions. This report examines three emerging leaders - Fireworks, Together.ai, and Groq - who are reshaping the landscape through distinct approaches to performance, cost-effectiveness, and enterprise solutions. Each company brings unique strengths: Fireworks with its developer-friendly platform, Together.ai's focus on enterprise security and customization, and Groq's groundbreaking LPU technology delivering exceptional inference speeds.\")]}}\n",
      "\n",
      "\n",
      "{'write_final_sections': {'completed_sections': [Section(name='Conclusion', description='Summarizes the key findings and insights from the report into a concise summary table, distilling the main points from the in-depth analysis of each provider.', research=False, content=\"## Conclusion\\n\\nThe AI inference market shows clear differentiation among the three major emerging providers, each carving out distinct competitive advantages. Here's how they compare across key metrics:\\n\\n| Provider | Key Strength | Performance | Valuation | Target Market |\\n|----------|--------------|-------------|-----------|---------------|\\n| Groq | Speed | 300+ tokens/sec (Llama 2 70B) | $3.6B | Enterprise, high-performance computing |\\n| Together.ai | Enterprise Security | 0.33s TTFT (Llama 3.3 70B) | $3.3B | Large enterprises, customization needs |\\n| Fireworks | Cost-effectiveness | 193 tokens/sec (Llama 3.1 8B) | $552M | Startups, developers |\\n\\nThe market's rapid evolution suggests continued innovation and competition. Groq's specialized hardware approach demonstrates the potential for significant performance gains, while Together.ai's enterprise focus and Fireworks' developer-friendly pricing show the importance of market segmentation. As GPU prices continue to fall and demand for AI inference grows, we expect to see further specialization and optimization from these providers to maintain their competitive advantages.\")]}}\n",
      "\n",
      "\n",
      "{'compile_final_report': {'final_report': \"# The AI Inference Market: Examining Fireworks, Together.ai, and Groq\\n\\nThe AI inference market is experiencing unprecedented growth, driven by increasing enterprise adoption and technological innovations. As the market expands at an estimated 40-55% annually toward a projected $780-990 billion valuation by 2027, new players are challenging traditional GPU manufacturers with specialized solutions. This report examines three emerging leaders - Fireworks, Together.ai, and Groq - who are reshaping the landscape through distinct approaches to performance, cost-effectiveness, and enterprise solutions. Each company brings unique strengths: Fireworks with its developer-friendly platform, Together.ai's focus on enterprise security and customization, and Groq's groundbreaking LPU technology delivering exceptional inference speeds.\\n\\n## Market Overview and Context\\n\\nThe AI inference market is experiencing remarkable growth, driven by increasing investments in AI technologies and the rising adoption across industries. According to market projections, the total addressable market for AI-related hardware and software is expected to grow between 40% and 55% annually over the next three years, potentially reaching $780-990 billion by 2027 [1].\\n\\nThe market landscape is evolving rapidly with diverse players offering specialized solutions. Traditional GPU manufacturers like NVIDIA continue to dominate, while newer entrants like Fireworks AI, Together AI, and Groq are disrupting the space with innovative approaches [2]. Fireworks AI has positioned itself as a cost-effective solution for startups and developers, particularly excelling in chat models and image generation, while Together AI focuses on high-performance training and enterprise security [3].\\n\\nRecent innovations in the market include edge computing solutions and advanced inference systems designed for real-time insights. The growth is particularly fueled by the increasing demand for processing large volumes of image data in real-time and the expansion of generative AI applications [4]. As the market matures, competition is intensifying around key performance metrics like throughput, latency, and cost-effectiveness [5].\\n\\n### Sources\\n[1] AI's Trillion-Dollar Opportunity: https://www.bain.com/insights/ais-trillion-dollar-opportunity-tech-report-2024/\\n[2] AI Inference Market Forecast Report to 2030: https://finance.yahoo.com/news/ai-inference-market-forecast-report-082100788.html\\n[3] Fireworks AI vs Together AI: AI Performance, Scalability, and Cost: https://koonka.ai/fireworks-ai-vs-together-ai/\\n[4] AI Inference Server Market Size, Share | CAGR of 18.40%: https://market.us/report/ai-inference-server-market/\\n[5] ArtificialAnalysis.ai LLM Benchmark Doubles Axis - Groq: https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n\\n## Deep Dive: Fireworks\\n\\nFireworks AI has emerged as a significant player in the AI inference market since its 2022 launch, experiencing remarkable growth that led to a $552 million valuation by mid-2024 [1]. The company's platform stands out for its high-performance inference capabilities, powered by its proprietary FireAttention inference engine that supports text, image, and audio processing while maintaining HIPAA and SOC2 compliance [2].\\n\\nThe platform operates on a flexible pay-as-you-go model, charging based on token usage for serverless inference, GPU time for on-demand deployments, and training data volume for fine-tuning [3]. Their model offerings demonstrate impressive performance metrics, with Llama 3.1 8B and Mixtral 8x7B leading in output speed at 193 and 171 tokens per second respectively [4].\\n\\nThe company has shown strong market traction, with traffic increasing 100x within six months of launch. Their latest $52 million Series B funding round, led by Sequoia, brought total funding to $77 million [5]. Fireworks has demonstrated practical value for developers, as evidenced by Cursor's implementation of their custom Llama 3-70b model achieving 1000 tokens/sec for code generation use cases [6].\\n\\n### Sources\\n[1] https://entrepreneurialtales.com/how-this-ai-startup-secured-a-552m-valuation-backed-by-nvidia/\\n[2] https://www.helicone.ai/blog/llm-api-providers\\n[3] https://docs.fireworks.ai/faq/billing-pricing-usage/pricing/cost-structure\\n[4] https://artificialanalysis.ai/providers/fireworks\\n[5] https://tracxn.com/d/companies/fireworks/__ucWeLWhllYG-tQw61ZYBnPmuZYGc-t6Njqo1qsxplTQ\\n[6] https://fireworks.ai/blog/fireworks-ai-series-b-compound-ai\\n\\n## Deep Dive: Together.ai\\n\\nTogether.ai has emerged as a significant player in the AI infrastructure space, focusing on simplifying enterprise use of open-source large language models (LLMs). The company's growth has been remarkable, achieving $100M in annual recurring revenue (ARR) in less than 10 months and reaching an estimated $130M ARR in 2024, representing a 400% year-over-year increase [1][2].\\n\\nThe company's valuation has seen impressive growth, rising to $3.3 billion following recent funding rounds. Together.ai has secured total funding of $228.5 million across three rounds, demonstrating strong investor confidence [3][4]. CEO Vipul Ved Prakash emphasizes their focus on combining state-of-the-art open source models with high-performance infrastructure and advanced AI efficiency research [5].\\n\\nUnlike traditional GPU cloud providers, Together.ai benefits from the commoditization of compute, as falling GPU prices lower their cost basis while maintaining competitive per-token pricing. The company differentiates itself through superior developer experience and reliable inference across a wide range of open-source models [2]. Their platform specifically caters to enterprises seeking AI infrastructure solutions that support model customization and deployment [4].\\n\\n### Sources\\n[1] https://www.arr.club/signal/together-ai-arr-at-100m-in-less-than-10-months\\n[2] https://sacra.com/c/together-ai/\\n[3] https://tracxn.com/d/companies/together-ai/__fcIBLE0rJMeK3FAdcfzE0H41jE36bJd0FDBWalYo6cY\\n[4] https://www.allaboutai.com/ai-news/together-ai-reaches-3-3b-dollars-valuation-general-catalyst-led-funding/\\n[5] https://www.forbes.com/sites/esatdedezade/2025/02/21/together-ais-valuation-soars-to-33-billion-as-demand-for-ai-computing-grows/\\n\\n## Deep Dive: Groq\\n\\nGroq has emerged as a significant player in the AI inference market, distinguished by its groundbreaking LPU (Linear Processing Unit) Inference Engine technology. In benchmark tests, Groq's LPU demonstrated exceptional performance, achieving over 300 tokens per second per user on the Llama 2 70B model, outperforming other cloud providers by up to 18x in output tokens throughput [1][2]. Independent benchmarks by ArtificialAnalysis.ai confirmed Groq's superior performance, recording 241 tokens per second and 0.8-second response time for 100 output tokens [3].\\n\\nFounded in 2016 by former Google engineers, Groq has seen substantial growth, recently securing $640 million in Series D funding [4]. The company's current implied valuation stands at $3.6 billion, marking a 219% increase from its Series C valuation [5]. With an estimated annual revenue of $46.6 million and 315 employees [6], Groq is rapidly scaling its operations.\\n\\nGroq's deployment strategy includes plans to install over 108,000 LPUs manufactured by GlobalFoundries by Q1 2025, representing the largest AI inference compute deployment outside of hyperscalers [7]. The company has already attracted over 360,000 developers to GroqCloud™, where they can access various open models including Llama 3.1, Whisper Large V3, and Mixtral [7].\\n\\n### Sources\\n[1] https://medium.com/@giladam01/why-meta-ais-llama-3-running-on-groq-s-lpu-inference-engine-sets-a-new-benchmark-for-large-2da740415773\\n[2] https://groq.com/groq-lpu-inference-engine-crushes-first-public-llm-benchmark/\\n[3] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n[4] https://aimagazine.com/machine-learning/groq-the-ai-chip-startup-worth-us-2-8bn\\n[5] https://www.pminsights.com/insights/groq-seeks-more-capital-and-continues-to-establish-itself-as-a-key-ai-player\\n[6] https://compworth.com/company/groq\\n[7] https://groq.com/news_press/groq-raises-640m-to-meet-soaring-demand-for-fast-ai-inference/\\n\\n## Comparative Analysis and ARR Estimates\\n\\nA detailed performance comparison reveals distinct advantages for each provider in the AI inference market. Groq stands out with exceptional speed metrics, achieving 241 tokens per second for Llama 2 Chat (70B), more than double the speed of many competitors [1]. More recent benchmarks show even higher performance, with Groq's specialized Tensor Streaming Processor (TSP) delivering 500-700 tokens/second on large language models, representing a 5-10x improvement over NVIDIA's data center GPUs [2].\\n\\nFireworks AI positions itself as the most cost-effective option for inference, particularly for models like Mixtral 8x7B and Llama 3 8B. It excels in providing fast and affordable inference services, making it especially attractive for startups and developers seeking efficient solutions [3].\\n\\nTogether.ai demonstrates strong performance in latency metrics, achieving impressive Time To First Token (TTFT) scores of 0.33s with their Turbo offering for Llama 3.3 70B [4]. While Cerebras and Groq show superior output speeds with their custom hardware, factors like TTFT and total response time create a more complex performance picture, particularly when handling longer context lengths [5].\\n\\n### Sources\\n[1] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\\n[2] https://sacra.com/c/groq/\\n[3] https://koonka.ai/fireworks-ai-vs-together-ai/\\n[4] https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers\\n[5] https://medium.com/friendliai/a-comparative-analysis-of-ai-api-providers-based-on-llama-3-1-70b-a9d89c52bfd1\\n\\n## Conclusion\\n\\nThe AI inference market shows clear differentiation among the three major emerging providers, each carving out distinct competitive advantages. Here's how they compare across key metrics:\\n\\n| Provider | Key Strength | Performance | Valuation | Target Market |\\n|----------|--------------|-------------|-----------|---------------|\\n| Groq | Speed | 300+ tokens/sec (Llama 2 70B) | $3.6B | Enterprise, high-performance computing |\\n| Together.ai | Enterprise Security | 0.33s TTFT (Llama 3.3 70B) | $3.3B | Large enterprises, customization needs |\\n| Fireworks | Cost-effectiveness | 193 tokens/sec (Llama 3.1 8B) | $552M | Startups, developers |\\n\\nThe market's rapid evolution suggests continued innovation and competition. Groq's specialized hardware approach demonstrates the potential for significant performance gains, while Together.ai's enterprise focus and Fireworks' developer-friendly pricing show the importance of market segmentation. As GPU prices continue to fall and demand for AI inference grows, we expect to see further specialization and optimization from these providers to maintain their competitive advantages.\"}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pass True to approve the report plan \n",
    "async for event in graph.astream(Command(resume=True), thread, stream_mode=\"updates\"):\n",
    "    print(event)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# AI Inference Market: Analysis of Fireworks, Together.ai, and Groq\n",
       "\n",
       "The AI inference market is experiencing rapid evolution as companies compete to provide faster, more efficient solutions for deploying and managing AI models. This analysis examines three key players reshaping the landscape: Fireworks, Together.ai, and Groq. Each brings distinct advantages - Fireworks with its specialized features and security focus, Together.ai with its comprehensive platform and impressive growth trajectory, and Groq with its groundbreaking Language Processing Unit technology. As organizations increasingly rely on AI inference capabilities, understanding these providers' strengths and market positions becomes crucial for making informed deployment decisions.\n",
       "\n",
       "## Fireworks AI Overview\n",
       "\n",
       "Fireworks AI has established itself as a significant player in the AI inference market, offering a comprehensive platform for deploying and managing AI models. The company provides access to various high-performance models, including Llama 3.1 405B and Qwen2.5 Coder 32B, which represent their highest quality offerings [1]. Their platform demonstrates impressive performance metrics, with some models achieving speeds up to 300 tokens per second for serverless deployments [2].\n",
       "\n",
       "The company has gained substantial market traction, recently securing $52 million in Series B funding that values the company at $552 million [3]. With an annual revenue of $6 million and a team of 60 employees, Fireworks AI serves notable clients including DoorDash, Quora, and Upwork [4, 5].\n",
       "\n",
       "Their platform differentiates itself through specialized features like FP8 quantization for large models and continuous batching capabilities [6]. Fireworks claims to offer approximately 3x faster speeds compared to competitors like Hugging Face TGI when using the same GPU configuration [2]. The company focuses on providing smaller, production-grade models that can be deployed privately and securely, rather than generic mega models [5].\n",
       "\n",
       "### Sources\n",
       "[1] Fireworks: Models Intelligence, Performance & Price: https://artificialanalysis.ai/providers/fireworks\n",
       "[2] Fireworks Platform Spring 2024 Updates: https://fireworks.ai/blog/spring-update-faster-models-dedicated-deployments-postpaid-pricing\n",
       "[3] Fireworks AI Raises $52M in Series B Funding: https://siliconvalleyjournals.com/fireworks-ai-raises-52m-in-series-b-funding-to-expand-genai-inference-platform/\n",
       "[4] Fireworks AI: Contact Details, Revenue, Funding: https://siliconvalleyjournals.com/company/fireworks-ai/\n",
       "[5] Fireworks AI Valued at $552 Million After New Funding Round: https://www.pymnts.com/news/investment-tracker/2024/fireworks-ai-valued-552-million-dollars-after-new-funding-round/\n",
       "[6] Inference performance - Fireworks AI Docs: https://docs.fireworks.ai/faq/models/inference/performance\n",
       "\n",
       "## Together.ai Overview\n",
       "\n",
       "Together.ai is a comprehensive AI acceleration platform that has quickly emerged as a significant player in the AI inference space since its founding in 2022 [1]. The platform enables developers to access over 200 AI models, offering high-performance inference capabilities with optimized infrastructure [2].\n",
       "\n",
       "The company's Inference Engine, built on NVIDIA Tensor Core GPUs, delivers impressive performance metrics, achieving 117 tokens per second on Llama-2-70B-Chat models [3]. Their model offerings include high-quality options like Llama 3.3 70B Turbo and Llama 3.1 405B Turbo, with some models achieving sub-100ms latency [4].\n",
       "\n",
       "Together.ai has demonstrated remarkable growth, with estimates suggesting $130M in annualized recurring revenue (ARR) in 2024, representing a 400% year-over-year increase [5]. The platform has received positive user feedback, maintaining a 4.8/5.0 rating based on 162 user reviews [6].\n",
       "\n",
       "The company differentiates itself through competitive pricing and technical innovations, incorporating advanced features like token caching, load balancing, and model quantization [2]. Users particularly praise the platform's straightforward API, reliable performance, and competitive pricing compared to alternatives [7].\n",
       "\n",
       "### Sources\n",
       "[1] https://siliconvalleyjournals.com/company/together-ai/\n",
       "[2] https://www.keywordsai.co/blog/top-10-llm-api-providers\n",
       "[3] https://www.together.ai/blog/together-inference-engine-v1\n",
       "[4] https://artificialanalysis.ai/providers/togetherai\n",
       "[5] https://sacra.com/c/together-ai/\n",
       "[6] https://www.featuredcustomers.com/vendor/together-ai\n",
       "[7] https://www.trustpilot.com/review/together.ai\n",
       "\n",
       "## Groq AI Overview\n",
       "\n",
       "Groq has emerged as a significant player in the AI inference market with its innovative Language Processing Unit (LPU) technology. Independent benchmarks from ArtificialAnalysis.ai demonstrate Groq's impressive performance, with their Llama 2 Chat (70B) API achieving 241 tokens per second - more than double the speed of competing providers [1]. Their total response time for 100 output tokens is just 0.8 seconds, positioning them as a leader in inference speed [2].\n",
       "\n",
       "The company's core technology, the Tensor Streaming Processor (TSP), delivers 500-700 tokens per second on large language models, representing a 5-10x improvement over Nvidia's latest data center GPUs [3]. This performance advantage has positioned Groq as a compelling alternative in the inference market, particularly for startups seeking cost-effective solutions [4].\n",
       "\n",
       "With approximately 300 employees, 60% of whom are software engineers, Groq has evolved beyond hardware to become a comprehensive AI solutions provider [5]. The company is strategically positioned to capture a share of the growing inference chip market, which is projected to reach $48 billion by 2027 [6].\n",
       "\n",
       "### Sources\n",
       "[1] https://groq.com/artificialanalysis-ai-llm-benchmark-doubles-axis-to-fit-new-groq-lpu-inference-engine-performance-results/\n",
       "[2] https://groq.com/inference/\n",
       "[3] https://sacra.com/c/groq/\n",
       "[4] https://venturebeat.com/ai/ai-chip-race-groq-ceo-takes-on-nvidia-claims-most-startups-will-use-speedy-lpus-by-end-of-2024/\n",
       "[5] https://www.businessinsider.com/groq-nvidia-software-advantage-cuda-moat-challenge-inference-2024-12?op=1\n",
       "[6] https://generativeaitech.substack.com/p/groq-and-its-impact-on-ai-inference\n",
       "\n",
       "## Market Comparison and Trends\n",
       "\n",
       "The AI inference market shows distinct positioning among key players Fireworks, Together.ai, and Groq. Fireworks AI differentiates itself through superior speed, leveraging its proprietary FireAttention inference engine for text, image, and audio processing, while maintaining HIPAA and SOC2 compliance for data privacy [1]. Together.ai has demonstrated remarkable growth, reaching an estimated $130M in annualized recurring revenue (ARR) in 2024, representing a 400% year-over-year increase [2].\n",
       "\n",
       "Both Fireworks and Together.ai focus on providing high-quality open-source models that can compete with proprietary alternatives [1]. Pricing structures vary across providers, with model costs ranging from $0.20 to $3.00 per million tokens for different model sizes and capabilities [3]. Together.ai's business model benefits from GPU price commoditization, allowing them to maintain competitive token pricing while focusing on developer experience and reliable inference across diverse open-source models [2].\n",
       "\n",
       "Groq distinguishes itself by offering quick access to various open-source AI models from major providers like Google, Meta, OpenAI, and Mistral through the OpenRouter API platform [4]. The market shows a trend toward comprehensive platform offerings, with providers competing on factors such as model variety, inference speed, and specialized features for different use cases.\n",
       "\n",
       "### Sources\n",
       "[1] Top 10 AI Inference Platforms in 2025: Comparing LLM API Providers: https://www.helicone.ai/blog/llm-api-providers\n",
       "[2] Together AI revenue, valuation & growth rate | Sacra: https://sacra.com/c/together-ai/\n",
       "[3] Models Leaderboard : Comparison of AI Models & API Providers - Groq AI: https://groq-ai.com/models/\n",
       "[4] Pricing: Compare Groq API Pricing With Other API Providers: https://groq-ai.com/pricing/\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The AI inference market shows clear differentiation among key players, with each provider carving out unique value propositions. Together.ai leads in revenue growth with $130M ARR, while Fireworks AI demonstrates strong potential with its recent $552M valuation. Groq distinguishes itself through superior inference speeds, achieving up to 700 tokens per second with its LPU technology.\n",
       "\n",
       "| Metric | Fireworks | Together.ai | Groq |\n",
       "|--------|-----------|-------------|------|\n",
       "| ARR | $6M | $130M | Not disclosed |\n",
       "| Speed (tokens/sec) | Up to 300 | Up to 117 | 500-700 |\n",
       "| Key Differentiator | FP8 quantization | 200+ model access | LPU technology |\n",
       "| Notable Feature | 3x faster than competitors | Token caching | Sub-second response |\n",
       "| Target Market | Enterprise security | Developer platforms | Startup solutions |\n",
       "\n",
       "The market trends indicate a shift toward comprehensive platform offerings that balance speed, cost-efficiency, and model variety. As the inference chip market approaches $48B by 2027, providers focusing on specialized hardware solutions and optimized infrastructure will likely gain competitive advantages. Success will depend on maintaining the delicate balance between performance, pricing, and platform features while addressing growing enterprise demands for security and reliability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "Markdown(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-deep-research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
